{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmoothQuant on OPT-13B\n",
    "\n",
    "### Guangxuan Xiao\\*, Ji Lin\\*, Mickael Seznec, Julien Demouth, Song Han\n",
    "\n",
    "In this notebook, we use OPT-13B model to demonstrate SmoothQuant can use 8-bit for both weights and activations to achieve the same accuracy as FP16 models. Unlike previous method [[Dettmers *et al.*, 2022]](https://arxiv.org/abs/2208.07339), SmoothQuant enables fully INT8 GEMMs for linear layers and does not require high precision numbers to represent outliers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates SmoothQuant on OPT-13B in consideration of the user's resouce constraints. We have tested SmoothQuant on up to 176 billion parameter models (OPT-175B, BLOOM-176B, GLM-130B). You can also adjust the model name to validate SmoothQuant on other models. `../act_scales/` provides the activation channel scales for OPT and BLOOM models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, you need to install the following packages:\n",
    "\n",
    "- smoothquant\n",
    "- PyTorch\n",
    "- Transformers\n",
    "- Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonic/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.models.opt.modeling_opt import OPTAttention, OPTDecoderLayer, OPTForCausalLM\n",
    "from transformers import GPT2Tokenizer\n",
    "from smoothquant.smooth import smooth_lm\n",
    "from smoothquant.fake_quant import W8A8Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ByobNet(\n",
    "  (stem): ConvNormAct(\n",
    "    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "    (bn): BatchNormAct2d(\n",
    "      16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "      (drop): Identity()\n",
    "      (act): SiLU(inplace=True)\n",
    "    )\n",
    "  )\n",
    "  (stages): Sequential(\n",
    "    (0): Sequential(\n",
    "      (0): BottleneckBlock(\n",
    "        (shortcut): Identity()\n",
    "        (conv1_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2_kxk): ConvNormAct(\n",
    "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2b_kxk): Identity()\n",
    "        (attn): Identity()\n",
    "        (conv3_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): Identity()\n",
    "          )\n",
    "        )\n",
    "        (attn_last): Identity()\n",
    "        (drop_path): Identity()\n",
    "        (act): Identity()\n",
    "      )\n",
    "    )\n",
    "    (1): Sequential(\n",
    "      (0): BottleneckBlock(\n",
    "        (conv1_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2_kxk): ConvNormAct(\n",
    "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2b_kxk): Identity()\n",
    "        (attn): Identity()\n",
    "        (conv3_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): Identity()\n",
    "          )\n",
    "        )\n",
    "        (attn_last): Identity()\n",
    "        (drop_path): Identity()\n",
    "        (act): Identity()\n",
    "      )\n",
    "      (1): BottleneckBlock(\n",
    "        (shortcut): Identity()\n",
    "        (conv1_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2_kxk): ConvNormAct(\n",
    "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2b_kxk): Identity()\n",
    "        (attn): Identity()\n",
    "        (conv3_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): Identity()\n",
    "          )\n",
    "        )\n",
    "        (attn_last): Identity()\n",
    "        (drop_path): Identity()\n",
    "        (act): Identity()\n",
    "      )\n",
    "      (2): BottleneckBlock(\n",
    "        (shortcut): Identity()\n",
    "        (conv1_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2_kxk): ConvNormAct(\n",
    "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2b_kxk): Identity()\n",
    "        (attn): Identity()\n",
    "        (conv3_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): Identity()\n",
    "          )\n",
    "        )\n",
    "        (attn_last): Identity()\n",
    "        (drop_path): Identity()\n",
    "        (act): Identity()\n",
    "      )\n",
    "    )\n",
    "    (2): Sequential(\n",
    "      (0): BottleneckBlock(\n",
    "        (conv1_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2_kxk): ConvNormAct(\n",
    "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2b_kxk): Identity()\n",
    "        (attn): Identity()\n",
    "        (conv3_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): Identity()\n",
    "          )\n",
    "        )\n",
    "        (attn_last): Identity()\n",
    "        (drop_path): Identity()\n",
    "        (act): Identity()\n",
    "      )\n",
    "      (1): MobileVitBlock(\n",
    "        (conv_kxk): ConvNormAct(\n",
    "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv_1x1): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (transformer): Sequential(\n",
    "          (0): Block(\n",
    "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
    "            (attn): Attention(\n",
    "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
    "              (q_norm): Identity()\n",
    "              (k_norm): Identity()\n",
    "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
    "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls1): Identity()\n",
    "            (drop_path1): Identity()\n",
    "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
    "            (mlp): Mlp(\n",
    "              (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
    "              (act): SiLU()\n",
    "              (drop1): Dropout(p=0.0, inplace=False)\n",
    "              (norm): Identity()\n",
    "              (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
    "              (drop2): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls2): Identity()\n",
    "            (drop_path2): Identity()\n",
    "          )\n",
    "          (1): Block(\n",
    "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
    "            (attn): Attention(\n",
    "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
    "              (q_norm): Identity()\n",
    "              (k_norm): Identity()\n",
    "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
    "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls1): Identity()\n",
    "            (drop_path1): Identity()\n",
    "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
    "            (mlp): Mlp(\n",
    "              (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
    "              (act): SiLU()\n",
    "              (drop1): Dropout(p=0.0, inplace=False)\n",
    "              (norm): Identity()\n",
    "              (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
    "              (drop2): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls2): Identity()\n",
    "            (drop_path2): Identity()\n",
    "          )\n",
    "        )\n",
    "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
    "        (conv_proj): ConvNormAct(\n",
    "          (conv): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv_fusion): ConvNormAct(\n",
    "          (conv): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (3): Sequential(\n",
    "      (0): BottleneckBlock(\n",
    "        (conv1_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2_kxk): ConvNormAct(\n",
    "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2b_kxk): Identity()\n",
    "        (attn): Identity()\n",
    "        (conv3_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): Identity()\n",
    "          )\n",
    "        )\n",
    "        (attn_last): Identity()\n",
    "        (drop_path): Identity()\n",
    "        (act): Identity()\n",
    "      )\n",
    "      (1): MobileVitBlock(\n",
    "        (conv_kxk): ConvNormAct(\n",
    "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv_1x1): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (transformer): Sequential(\n",
    "          (0): Block(\n",
    "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
    "            (attn): Attention(\n",
    "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
    "              (q_norm): Identity()\n",
    "              (k_norm): Identity()\n",
    "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
    "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls1): Identity()\n",
    "            (drop_path1): Identity()\n",
    "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
    "            (mlp): Mlp(\n",
    "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
    "              (act): SiLU()\n",
    "              (drop1): Dropout(p=0.0, inplace=False)\n",
    "              (norm): Identity()\n",
    "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
    "              (drop2): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls2): Identity()\n",
    "            (drop_path2): Identity()\n",
    "          )\n",
    "          (1): Block(\n",
    "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
    "            (attn): Attention(\n",
    "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
    "              (q_norm): Identity()\n",
    "              (k_norm): Identity()\n",
    "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
    "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls1): Identity()\n",
    "            (drop_path1): Identity()\n",
    "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
    "            (mlp): Mlp(\n",
    "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
    "              (act): SiLU()\n",
    "              (drop1): Dropout(p=0.0, inplace=False)\n",
    "              (norm): Identity()\n",
    "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
    "              (drop2): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls2): Identity()\n",
    "            (drop_path2): Identity()\n",
    "          )\n",
    "          (2): Block(\n",
    "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
    "            (attn): Attention(\n",
    "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
    "              (q_norm): Identity()\n",
    "              (k_norm): Identity()\n",
    "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
    "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls1): Identity()\n",
    "            (drop_path1): Identity()\n",
    "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
    "            (mlp): Mlp(\n",
    "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
    "              (act): SiLU()\n",
    "              (drop1): Dropout(p=0.0, inplace=False)\n",
    "              (norm): Identity()\n",
    "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
    "              (drop2): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls2): Identity()\n",
    "            (drop_path2): Identity()\n",
    "          )\n",
    "          (3): Block(\n",
    "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
    "            (attn): Attention(\n",
    "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
    "              (q_norm): Identity()\n",
    "              (k_norm): Identity()\n",
    "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
    "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls1): Identity()\n",
    "            (drop_path1): Identity()\n",
    "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
    "            (mlp): Mlp(\n",
    "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
    "              (act): SiLU()\n",
    "              (drop1): Dropout(p=0.0, inplace=False)\n",
    "              (norm): Identity()\n",
    "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
    "              (drop2): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls2): Identity()\n",
    "            (drop_path2): Identity()\n",
    "          )\n",
    "        )\n",
    "        (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
    "        (conv_proj): ConvNormAct(\n",
    "          (conv): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv_fusion): ConvNormAct(\n",
    "          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (4): Sequential(\n",
    "      (0): BottleneckBlock(\n",
    "        (conv1_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2_kxk): ConvNormAct(\n",
    "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv2b_kxk): Identity()\n",
    "        (attn): Identity()\n",
    "        (conv3_1x1): ConvNormAct(\n",
    "          (conv): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): Identity()\n",
    "          )\n",
    "        )\n",
    "        (attn_last): Identity()\n",
    "        (drop_path): Identity()\n",
    "        (act): Identity()\n",
    "      )\n",
    "      (1): MobileVitBlock(\n",
    "        (conv_kxk): ConvNormAct(\n",
    "          (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv_1x1): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (transformer): Sequential(\n",
    "          (0): Block(\n",
    "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
    "            (attn): Attention(\n",
    "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
    "              (q_norm): Identity()\n",
    "              (k_norm): Identity()\n",
    "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
    "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls1): Identity()\n",
    "            (drop_path1): Identity()\n",
    "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
    "            (mlp): Mlp(\n",
    "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
    "              (act): SiLU()\n",
    "              (drop1): Dropout(p=0.0, inplace=False)\n",
    "              (norm): Identity()\n",
    "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
    "              (drop2): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls2): Identity()\n",
    "            (drop_path2): Identity()\n",
    "          )\n",
    "          (1): Block(\n",
    "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
    "            (attn): Attention(\n",
    "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
    "              (q_norm): Identity()\n",
    "              (k_norm): Identity()\n",
    "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
    "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls1): Identity()\n",
    "            (drop_path1): Identity()\n",
    "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
    "            (mlp): Mlp(\n",
    "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
    "              (act): SiLU()\n",
    "              (drop1): Dropout(p=0.0, inplace=False)\n",
    "              (norm): Identity()\n",
    "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
    "              (drop2): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls2): Identity()\n",
    "            (drop_path2): Identity()\n",
    "          )\n",
    "          (2): Block(\n",
    "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
    "            (attn): Attention(\n",
    "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
    "              (q_norm): Identity()\n",
    "              (k_norm): Identity()\n",
    "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
    "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls1): Identity()\n",
    "            (drop_path1): Identity()\n",
    "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
    "            (mlp): Mlp(\n",
    "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
    "              (act): SiLU()\n",
    "              (drop1): Dropout(p=0.0, inplace=False)\n",
    "              (norm): Identity()\n",
    "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
    "              (drop2): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (ls2): Identity()\n",
    "            (drop_path2): Identity()\n",
    "          )\n",
    "        )\n",
    "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
    "        (conv_proj): ConvNormAct(\n",
    "          (conv): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "        (conv_fusion): ConvNormAct(\n",
    "          (conv): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "          (bn): BatchNormAct2d(\n",
    "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "            (drop): Identity()\n",
    "            (act): SiLU(inplace=True)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (final_conv): ConvNormAct(\n",
    "    (conv): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    (bn): BatchNormAct2d(\n",
    "      320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
    "      (drop): Identity()\n",
    "      (act): SiLU(inplace=True)\n",
    "    )\n",
    "  )\n",
    "  (head): ClassifierHead(\n",
    "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
    "    (drop): Dropout(p=0.0, inplace=False)\n",
    "    (fc): Linear(in_features=320, out_features=1000, bias=True)\n",
    "    (flatten): Identity()\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByobNet(\n",
       "  (stem): ConvNormAct(\n",
       "    (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNormAct2d(\n",
       "      16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (shortcut): Identity()\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "      (1): BottleneckBlock(\n",
       "        (shortcut): Identity()\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "      (2): BottleneckBlock(\n",
       "        (shortcut): Identity()\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "      (1): MobileVitBlock(\n",
       "        (conv_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_1x1): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (transformer): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_proj): ConvNormAct(\n",
       "          (conv): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_fusion): ConvNormAct(\n",
       "          (conv): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "      (1): MobileVitBlock(\n",
       "        (conv_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_1x1): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (transformer): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=80, out_features=240, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=80, out_features=160, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=160, out_features=80, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_proj): ConvNormAct(\n",
       "          (conv): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_fusion): ConvNormAct(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): BottleneckBlock(\n",
       "        (conv1_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2b_kxk): Identity()\n",
       "        (attn): Identity()\n",
       "        (conv3_1x1): ConvNormAct(\n",
       "          (conv): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attn_last): Identity()\n",
       "        (drop_path): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "      (1): MobileVitBlock(\n",
       "        (conv_kxk): ConvNormAct(\n",
       "          (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_1x1): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (transformer): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): Identity()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
       "              (act): SiLU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): Identity()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv_proj): ConvNormAct(\n",
       "          (conv): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_fusion): ConvNormAct(\n",
       "          (conv): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_conv): ConvNormAct(\n",
       "    (conv): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNormAct2d(\n",
       "      320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=320, out_features=1000, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "m = timm.create_model('mobilevit_xxs.cvnets_in1k', pretrained=True)\n",
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we simulate the 8-bit dynamic per-tensor weight and activation quantization with FP16, i.e., fake quantization. We have implemented the real 8-bit quantization with INT8 CUTLASS GEMM kernels for both PyTorch and FasterTransformer. Please stay tuned for the release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, weight_quant='per_tensor', act_quant='per_tensor', quantize_bmm_input=True):\n",
    "    for name, m in model.model.named_modules():\n",
    "        print('Name: ', name)\n",
    "        print('Module: ', m)\n",
    "        if isinstance(m, OPTDecoderLayer):\n",
    "            m.fc1 = W8A8Linear.from_float(m.fc1, weight_quant=weight_quant, act_quant=act_quant)\n",
    "            m.fc2 = W8A8Linear.from_float(m.fc2, weight_quant=weight_quant, act_quant=act_quant)\n",
    "        elif isinstance(m, OPTAttention):\n",
    "            # Her we simulate quantizing BMM inputs by quantizing the output of q_proj, k_proj, v_proj\n",
    "            m.q_proj = W8A8Linear.from_float(\n",
    "                m.q_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.k_proj = W8A8Linear.from_float(\n",
    "                m.k_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.v_proj = W8A8Linear.from_float(\n",
    "                m.v_proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input)\n",
    "            m.out_proj = W8A8Linear.from_float(m.out_proj, weight_quant=weight_quant, act_quant=act_quant)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an evaluator to see the performance of the model. We use a toy dataset (the first 1000 examples in the validation set of the Lambada dataset) to evaluate the model. You can replace it with your own dataset. The conclusion should be the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**In this demo, we have simplified the evaluation by using the first 1,000 samples from the LAMBADA dataset's validation set. We employ the \"Last Token Prediction Accuracy\" as our evaluation metric. This approximate evaluation is intended for demonstration purposes, providing simple but meaningful comparisons of relative performance between methods. For a more strict assessment, we recommend using the [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness) to obtain the \"Last Word Prediction Accuracy\" for the LAMBADA dataset, which is the reported metric in our paper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        # tokenize the dataset\n",
    "        def tokenize_function(examples):\n",
    "            example = self.tokenizer(examples['text'])\n",
    "            return example\n",
    "\n",
    "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
    "        self.dataset.set_format(type='torch', columns=['input_ids'])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        # The task is to predict the last word of the input.\n",
    "        total, hit = 0, 0\n",
    "        for batch in self.dataset:\n",
    "            input_ids = batch['input_ids'].to(self.device).unsqueeze(0)\n",
    "            label = input_ids[:, -1]\n",
    "            outputs = model(input_ids)\n",
    "            last_token_logits = outputs.logits[:, -2, :]\n",
    "            pred = last_token_logits.argmax(dim=-1)\n",
    "            total += label.size(0)\n",
    "            hit += (pred == label).sum().item()\n",
    "        acc = hit / total\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset lambada (/home/sonic/.cache/huggingface/datasets/lambada/plain_text/1.1.0/9f7bada20233bfec7d1d888d179c81442d504fb3d0dd97cddeba020b19924373)\n",
      "Loading cached processed dataset at /home/sonic/.cache/huggingface/datasets/lambada/plain_text/1.1.0/9f7bada20233bfec7d1d888d179c81442d504fb3d0dd97cddeba020b19924373/cache-85ec25cb51103112.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('facebook/opt-13b')\n",
    "dataset = load_dataset('lambada', split='validation[:1000]')\n",
    "evaluator = Evaluator(dataset, tokenizer, 'cuda')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Model Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the performance of the original FP16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 3/3 [00:16<00:00,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 5120, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (12): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (13): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (14): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (15): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (16): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (17): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (18): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (19): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (20): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (21): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (22): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (23): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (24): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (25): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (26): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (27): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (28): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (29): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (30): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (31): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (32): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (33): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (34): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (35): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (36): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (37): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (38): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (39): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "          (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = OPTForCausalLM.from_pretrained('facebook/opt-13b', torch_dtype=torch.float16, device_map='auto')\n",
    "print(model_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model (fp16) accuracy: 0.786\n"
     ]
    }
   ],
   "source": [
    "acc_fp16 = evaluator.evaluate(model_fp16)\n",
    "print(f'Original model (fp16) accuracy: {acc_fp16}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then quantize the model to W8A8 and check the performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive W8A8 Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  \n",
      "Module:  OPTModel(\n",
      "  (decoder): OPTDecoder(\n",
      "    (embed_tokens): Embedding(50272, 5120, padding_idx=1)\n",
      "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (12): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (13): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (14): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (15): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (16): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (17): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (18): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (19): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (20): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (21): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (22): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (23): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (24): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (25): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (26): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (27): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (28): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (29): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (30): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (31): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (32): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (33): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (34): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (35): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (36): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (37): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (38): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (39): OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "        (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Name:  decoder\n",
      "Module:  OPTDecoder(\n",
      "  (embed_tokens): Embedding(50272, 5120, padding_idx=1)\n",
      "  (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (layers): ModuleList(\n",
      "    (0): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (9): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (10): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (11): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (12): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (13): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (14): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (15): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (16): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (17): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (18): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (19): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (20): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (21): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (22): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (23): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (24): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (25): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (26): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (27): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (28): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (29): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (30): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (31): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (32): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (33): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (34): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (35): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (36): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (37): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (38): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (39): OPTDecoderLayer(\n",
      "      (self_attn): OPTAttention(\n",
      "        (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "      (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Name:  decoder.embed_tokens\n",
      "Module:  Embedding(50272, 5120, padding_idx=1)\n",
      "Name:  decoder.embed_positions\n",
      "Module:  OPTLearnedPositionalEmbedding(2050, 5120)\n",
      "Name:  decoder.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers\n",
      "Module:  ModuleList(\n",
      "  (0): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (1): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (2): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (3): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (4): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (5): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (6): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (7): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (8): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (9): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (10): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (11): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (12): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (13): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (14): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (15): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (16): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (17): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (18): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (19): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (20): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (21): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (22): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (23): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (24): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (25): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (26): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (27): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (28): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (29): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (30): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (31): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (32): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (33): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (34): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (35): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (36): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (37): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (38): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (39): OPTDecoderLayer(\n",
      "    (self_attn): OPTAttention(\n",
      "      (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "      (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "    (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "Name:  decoder.layers.0\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.0.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.0.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.0.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.0.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.0.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.0.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.0.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.0.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.0.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.0.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.1\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.1.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.1.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.1.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.1.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.1.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.1.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.1.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.1.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.1.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.1.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.2\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.2.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.2.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.2.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.2.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.2.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.2.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.2.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.2.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.2.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.2.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.3\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.3.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.3.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.3.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.3.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.3.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.3.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.3.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.3.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.3.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.3.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.4\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.4.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.4.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.4.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.4.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.4.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.4.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.4.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.4.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.4.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.4.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.5\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.5.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.5.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.5.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.5.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.5.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.5.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.5.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.5.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.5.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.5.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.6\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.6.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.6.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.6.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.6.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.6.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.6.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.6.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.6.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.6.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.6.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.7\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.7.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.7.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.7.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.7.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.7.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.7.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.7.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.7.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.7.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.7.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.8\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.8.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.8.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.8.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.8.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.8.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.8.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.8.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.8.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.8.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.8.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.9\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.9.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.9.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.9.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.9.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.9.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.9.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.9.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.9.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.9.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.9.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.10\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.10.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.10.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.10.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.10.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.10.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.10.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.10.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.10.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.10.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.10.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.11\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.11.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.11.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.11.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.11.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.11.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.11.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.11.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.11.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.11.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.11.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.12\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.12.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.12.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.12.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.12.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.12.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.12.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.12.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.12.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.12.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.12.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.13\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.13.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.13.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.13.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.13.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.13.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.13.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.13.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.13.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.13.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.13.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.14\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.14.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.14.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.14.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.14.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.14.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.14.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.14.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.14.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.14.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.14.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.15\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.15.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.15.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.15.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.15.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.15.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.15.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.15.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.15.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.15.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.15.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.16\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.16.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.16.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.16.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.16.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.16.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.16.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.16.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.16.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.16.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.16.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.17\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.17.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.17.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.17.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.17.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.17.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.17.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.17.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.17.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.17.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.17.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.18\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.18.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.18.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.18.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.18.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.18.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.18.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.18.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.18.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.18.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.18.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.19\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.19.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.19.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.19.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.19.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.19.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.19.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.19.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.19.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.19.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.19.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.20\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.20.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.20.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.20.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.20.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.20.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.20.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.20.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.20.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.20.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.20.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.21\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.21.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.21.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.21.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.21.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.21.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.21.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.21.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.21.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.21.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.21.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.22\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.22.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.22.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.22.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.22.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.22.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.22.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.22.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.22.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.22.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.22.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.23\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.23.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.23.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.23.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.23.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.23.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.23.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.23.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.23.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.23.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.23.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.24\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.24.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.24.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.24.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.24.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.24.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.24.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.24.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.24.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.24.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.24.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.25\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.25.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.25.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.25.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.25.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.25.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.25.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.25.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.25.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.25.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.25.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.26\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.26.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.26.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.26.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.26.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.26.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.26.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.26.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.26.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.26.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.26.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.27\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.27.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.27.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.27.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.27.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.27.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.27.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.27.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.27.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.27.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.27.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.28\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.28.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.28.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.28.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.28.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.28.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.28.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.28.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.28.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.28.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.28.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.29\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.29.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.29.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.29.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.29.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.29.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.29.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.29.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.29.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.29.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.29.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.30\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.30.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.30.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.30.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.30.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.30.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.30.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.30.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.30.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.30.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.30.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.31\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.31.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.31.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.31.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.31.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.31.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.31.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.31.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.31.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.31.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.31.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.32\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.32.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.32.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.32.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.32.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.32.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.32.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.32.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.32.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.32.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.32.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.33\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.33.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.33.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.33.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.33.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.33.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.33.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.33.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.33.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.33.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.33.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.34\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.34.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.34.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.34.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.34.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.34.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.34.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.34.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.34.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.34.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.34.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.35\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.35.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.35.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.35.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.35.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.35.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.35.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.35.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.35.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.35.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.35.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.36\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.36.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.36.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.36.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.36.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.36.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.36.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.36.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.36.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.36.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.36.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.37\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.37.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.37.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.37.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.37.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.37.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.37.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.37.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.37.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.37.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.37.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.38\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.38.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.38.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.38.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.38.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.38.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.38.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.38.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.38.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.38.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.38.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.39\n",
      "Module:  OPTDecoderLayer(\n",
      "  (self_attn): OPTAttention(\n",
      "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
      "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
      "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Name:  decoder.layers.39.self_attn\n",
      "Module:  OPTAttention(\n",
      "  (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "  (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      ")\n",
      "Name:  decoder.layers.39.self_attn.k_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.39.self_attn.v_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.39.self_attn.q_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "Name:  decoder.layers.39.self_attn.out_proj\n",
      "Module:  W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.39.activation_fn\n",
      "Module:  ReLU()\n",
      "Name:  decoder.layers.39.self_attn_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "Name:  decoder.layers.39.fc1\n",
      "Module:  W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.39.fc2\n",
      "Module:  W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "Name:  decoder.layers.39.final_layer_norm\n",
      "Module:  LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 5120, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (12): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (13): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (14): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (15): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (16): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (17): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (18): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (19): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (20): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (21): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (22): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (23): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (24): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (25): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (26): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (27): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (28): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (29): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (30): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (31): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (32): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (33): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (34): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (35): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (36): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (37): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (38): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (39): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_w8a8 = quantize_model(model_fp16)\n",
    "print(model_w8a8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive W8A8 quantized model accuracy: 0.048\n"
     ]
    }
   ],
   "source": [
    "acc_w8a8 = evaluator.evaluate(model_w8a8)\n",
    "print(f'Naive W8A8 quantized model accuracy: {acc_w8a8}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is a significant accuracy drop. This is consistent with LLM.int8()'s finding: when the model size increases larger than 6.7B, systematic outliers will emerge in activations, which makes fully INT8 quantization impossible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothQuant W8A8 Quantized Model Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's smooth the model, quantize it, and check the performance! In `../act_scales`, we provide the activation scales for OPT and BLOOM models. You can also use this notebook to test quantizing those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 3/3 [00:17<00:00,  5.71s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 2&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>model = OPTForCausalLM.from_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">'facebook/opt-13b'</span>, torch_dtype=torch.float16, de     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>2 act_scales = torch.load(<span style=\"color: #808000; text-decoration-color: #808000\">'../act_scales/opt-13b.pt'</span>)                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>smooth_lm(model, act_scales, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.5</span>)                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>model_smoothquant_w8a8 = quantize_model(model)                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(model_smoothquant_w8a8)                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/anaconda3/envs/smoothquant/lib/python3.8/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">713</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 710                </span>opened_file.seek(orig_position)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 711                </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.jit.load(opened_file)                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 712             </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _load(opened_zipfile, map_location, pickle_module, **pickle_load_  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 713 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 714 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 715 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 716 # Register pickling support for layout instances such as</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/anaconda3/envs/smoothquant/lib/python3.8/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">920</span> in        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_legacy_load</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 917          </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Received object of type \\\"{</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(f)<span style=\"color: #808000; text-decoration-color: #808000\">}\\\". Please update to Python 3.8.2 or ne</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 918          </span><span style=\"color: #808000; text-decoration-color: #808000\">\"functionality.\"</span>)                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 919    </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 920 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>magic_number = pickle_module.load(f, **pickle_load_args)                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 921    </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> magic_number != MAGIC_NUMBER:                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 922       </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Invalid magic number; corrupt file?\"</span>)                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 923    </span>protocol_version = pickle_module.load(f, **pickle_load_args)                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">UnpicklingError: </span>invalid load key, <span style=\"color: #008000; text-decoration-color: #008000\">'v'</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<cell line: 2>\u001b[0m:\u001b[94m2\u001b[0m                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1 \u001b[0mmodel = OPTForCausalLM.from_pretrained(\u001b[33m'\u001b[0m\u001b[33mfacebook/opt-13b\u001b[0m\u001b[33m'\u001b[0m, torch_dtype=torch.float16, de     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m2 act_scales = torch.load(\u001b[33m'\u001b[0m\u001b[33m../act_scales/opt-13b.pt\u001b[0m\u001b[33m'\u001b[0m)                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3 \u001b[0msmooth_lm(model, act_scales, \u001b[94m0.5\u001b[0m)                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m4 \u001b[0mmodel_smoothquant_w8a8 = quantize_model(model)                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m5 \u001b[0m\u001b[96mprint\u001b[0m(model_smoothquant_w8a8)                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/anaconda3/envs/smoothquant/lib/python3.8/site-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m713\u001b[0m in \u001b[92mload\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 710 \u001b[0m\u001b[2m               \u001b[0mopened_file.seek(orig_position)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 711 \u001b[0m\u001b[2m               \u001b[0m\u001b[94mreturn\u001b[0m torch.jit.load(opened_file)                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 712 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mreturn\u001b[0m _load(opened_zipfile, map_location, pickle_module, **pickle_load_  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 713 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 714 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 715 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m# Register pickling support for layout instances such as\u001b[0m                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/opt/anaconda3/envs/smoothquant/lib/python3.8/site-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m920\u001b[0m in        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92m_legacy_load\u001b[0m                                                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 917 \u001b[0m\u001b[2m         \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mReceived object of type \u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(f)\u001b[33m}\u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m. Please update to Python 3.8.2 or ne\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 918 \u001b[0m\u001b[2m         \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mfunctionality.\u001b[0m\u001b[33m\"\u001b[0m)                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 919 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 920 \u001b[2m   \u001b[0mmagic_number = pickle_module.load(f, **pickle_load_args)                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 921 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mif\u001b[0m magic_number != MAGIC_NUMBER:                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInvalid magic number; corrupt file?\u001b[0m\u001b[33m\"\u001b[0m)                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m   \u001b[0mprotocol_version = pickle_module.load(f, **pickle_load_args)                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mUnpicklingError: \u001b[0minvalid load key, \u001b[32m'v'\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = OPTForCausalLM.from_pretrained('facebook/opt-13b', torch_dtype=torch.float16, device_map='auto')\n",
    "act_scales = torch.load('../act_scales/opt-13b.pt')\n",
    "smooth_lm(model, act_scales, 0.5)\n",
    "model_smoothquant_w8a8 = quantize_model(model)\n",
    "print(model_smoothquant_w8a8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the smoothed model has the same accuracy as the FP16 model. This is because SmoothQuant smooths the outliers in activations and moves the quantization difficulty from activations to weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1 acc_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f'SmoothQuant W8A8 quantized model accuracy: {</span>acc_smoothquant_w8a8<span style=\"color: #808000; text-decoration-color: #808000\">}'</span>)                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'model_smoothquant_w8a8'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1 acc_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m2 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mSmoothQuant W8A8 quantized model accuracy: \u001b[0m\u001b[33m{\u001b[0macc_smoothquant_w8a8\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m)                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'model_smoothquant_w8a8'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)\n",
    "print(f'SmoothQuant W8A8 quantized model accuracy: {acc_smoothquant_w8a8}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
